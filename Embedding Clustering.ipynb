{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of this notebook is to cluster and assign cluster numbers to each word so that they might be useful as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import basic\n",
    "from basic.nlp.tokenizers import clinical_tokenizers\n",
    "from basic.nlp.annotation.annotation import Annotation, AnnotatedDocument\n",
    "from basic.nlp.sequenceutils import get_sentence_bio_tagged_tokens\n",
    "\n",
    "from madetokenizer import build_made_tokenizer\n",
    "from madeutils import read_made_data, get_all_sentence_tokens_and_tags, gather_validation_metrics\n",
    "\n",
    "print('Imported custom BASIC modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_BASE_DIR = r'c:\\embeddings'\n",
    "\n",
    "CLUSTERS_BASE_DIR = r'resources/clusters'\n",
    "\n",
    "PRETRAINED_EMBEDDINGS_FILENAME = r'wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "#PRETRAINED_EMBEDDINGS_FILENAME = r'pubmed+wiki+pitts-nopunct-lower-cbow-n10.bin'\n",
    "\n",
    "K_CLUSTERS = 500\n",
    "ENABLED_BATCH_KMEANS = True\n",
    "KMEANS_BATCH_SIZE = 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load some pretrained embeddings as well\n",
    "\n",
    "# NOTE : These embeddings are made available here:\n",
    "# http://evexdb.org/pmresources/vec-space-models/\n",
    "\n",
    "pretrained_word_vectors = KeyedVectors.load_word2vec_format(os.path.join(EMBEDDINGS_BASE_DIR, PRETRAINED_EMBEDDINGS_FILENAME), binary=True)  # C binary format\n",
    "                                                 \n",
    "print(pretrained_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pretrained_word_vectors['the'].shape)\n",
    "\n",
    "pretrained_embeddings_dimensions = pretrained_word_vectors['the'].shape[0]\n",
    "print(pretrained_embeddings_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pretrained_word_vectors\n",
    "embeddings_dimensions = pretrained_embeddings_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "word_vectors = pretrained_word_vectors.syn0\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "print('Running K means')\n",
    "\n",
    "if ENABLED_BATCH_KMEANS:\n",
    "    print('Using batch KMeans')\n",
    "    kmeans = MiniBatchKMeans(n_clusters = K_CLUSTERS, \n",
    "                         #n_jobs = -2, \n",
    "                         batch_size = KMEANS_BATCH_SIZE)\n",
    "else:\n",
    "    print('Using original recipe KMeans')\n",
    "    kmeans = KMeans( n_clusters = K_CLUSTERS, n_jobs = -2 )\n",
    "\n",
    "cluster_idx = kmeans.fit_predict( word_vectors )\n",
    "\n",
    "print('K means trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number\n",
    "word_cluster_map = dict(zip(pretrained_word_vectors.wv.index2word, cluster_idx ))\n",
    "\n",
    "print(list(word_cluster_map.items())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_cluster_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typename = 'KMeans'\n",
    "if ENABLED_BATCH_KMEANS:\n",
    "    typename = 'BatchKmeans'\n",
    "\n",
    "map_pickle_file_name = '{3}/WordClusters_K{0}_{1}_{2}.pickle'.format(K_CLUSTERS, \n",
    "                                                                     typename, \n",
    "                                                                     PRETRAINED_EMBEDDINGS_FILENAME.split('.')[0],\n",
    "                                                                    CLUSTERS_BASE_DIR)\n",
    "\n",
    "print('Writing cluster map pickle to : {}'.format(map_pickle_file_name))\n",
    "      \n",
    "with open(map_pickle_file_name, 'wb') as handle:\n",
    "    pickle.dump(word_cluster_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      \n",
    "print('DONE writing cluster map pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
