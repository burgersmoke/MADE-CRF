{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of this notebook is to wire up evaluation from the MADE challenge using BIOC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import string\n",
    "from collections import Counter\n",
    "import glob\n",
    "import time\n",
    "import subprocess\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioc\n",
    "\n",
    "from bioc_evaluation import get_f_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV, ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite.utils import flatten\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import basic\n",
    "from basic.nlp.tokenizers import clinical_tokenizers\n",
    "from basic.nlp.annotation.annotation import Annotation, AnnotatedDocument\n",
    "from basic.nlp.sequenceutils import get_sentence_bio_tagged_tokens\n",
    "\n",
    "from madetokenizer import build_made_tokenizer\n",
    "from madeutils import read_made_data, get_all_sentence_tokens_and_tags, gather_validation_metrics\n",
    "from madeutils import get_coarse_labels, evaluate_via_bioc, plot_confusion_matrix\n",
    "\n",
    "print('Imported custom BASIC modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MADE_ALL_SETS_BASE_DIR = r'C:\\MADE'\n",
    "MADE_BASE_DIR = r'{0}\\MADE-1.0'.format(MADE_ALL_SETS_BASE_DIR)\n",
    "PREDICTION_DIR = 'predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERS_BASE_DIR = r'resources/clusters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDEX_DIR = r'resources/medex'\n",
    "drug_term_file = 'medex_filtered_terms.txt'\n",
    "drug_term_path = os.path.join(MEDEX_DIR, drug_term_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also load and test a Brill Part of Speech tagger which was trained on the Penn Treebank:\n",
    "BRILL_TAGGER_FILE_PATH = 'resources/pos/treebank_brill_aubt.pickle'\n",
    "brill_tagger = pickle.load(open(BRILL_TAGGER_FILE_PATH, 'rb'))\n",
    "print(brill_tagger)\n",
    "\n",
    "# now let's kick the tires on this tagger\n",
    "test_tag_tokens = 'The cat walked onto an airplane'.split()\n",
    "print(brill_tagger.tag(test_tag_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's load in the original training test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "made_tokenizer = build_made_tokenizer()\n",
    "annotated_docs = read_made_data(MADE_BASE_DIR, made_tokenizer)\n",
    "\n",
    "print('Total Annotated Docs : {}'.format(len(annotated_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To evaluate properly, we need to split apart by documents first so that we have the ability to track tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, test_docs = sklearn.model_selection.train_test_split(annotated_docs, train_size=0.8, random_state=RANDOM_STATE)\n",
    "print('Total training documents : {}'.format(len(train_docs)))\n",
    "print('Total test documents : {}'.format(len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# now let' pile up all of our documents and split them into an initial fold\n",
    "print('Fetching tokens and tags...')\n",
    "X_train, y_train = get_all_sentence_tokens_and_tags(train_docs)\n",
    "X_test, y_test = get_all_sentence_tokens_and_tags(test_docs)\n",
    "print('DONE Fetching tokens and tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRFExtractor(object):\n",
    "    def __init__(self,\n",
    "                 window_size = 2,\n",
    "                 char_ngrams_enabled = set([2, 3]),\n",
    "                enable_stem_features = False,\n",
    "                enable_pos_features = False,\n",
    "                 enable_lexical_features = True,\n",
    "                pos_tagger = None,\n",
    "                word_cluster_map_tuples = [],\n",
    "                enable_sentence_window_clusters = False,\n",
    "                enable_local_window_clusters = False,\n",
    "                 enable_cluster_bigrams = False,\n",
    "                drug_term_set = set(),\n",
    "                enable_drug_term_features = False):\n",
    "        print('Creating CRFExtractor')\n",
    "        self.window_size = window_size\n",
    "        self.char_ngrams_enabled = char_ngrams_enabled\n",
    "        self.enable_stem_features = enable_stem_features\n",
    "        self.enable_pos_features = enable_pos_features\n",
    "        self.enable_lexical_features = enable_lexical_features\n",
    "        self.pos_tagger = pos_tagger\n",
    "        self.word_cluster_map_tuples = word_cluster_map_tuples\n",
    "        self.enable_sentence_window_clusters = enable_sentence_window_clusters\n",
    "        self.enable_local_window_clusters = enable_local_window_clusters\n",
    "        self.enable_cluster_bigrams = enable_cluster_bigrams\n",
    "        self.drug_term_set = drug_term_set\n",
    "        self.enable_drug_term_features = enable_drug_term_features\n",
    "        \n",
    "        # this is used to collect drug terms found\n",
    "        self.found_drug_term_set = set()\n",
    "        \n",
    "        print('Total word maps loaded : {}'.format(len(self.word_cluster_map_tuples)))\n",
    "        print('Character ngrams enabled : {}'.format(str(char_ngrams_enabled)))\n",
    "        \n",
    "    def add_sentence_window_clusters(self, feature_prefix, tokens, features, cluster_mapping_tokens_map):\n",
    "        if len(self.word_cluster_map_tuples) == 0:\n",
    "            return\n",
    "        \n",
    "        # iterate through the list of mappings already computed\n",
    "        for map_prefix, token_clusters in cluster_mapping_tokens_map.items():\n",
    "            # loop through each token in the window and add a one-hot value for each cluster mapped\n",
    "            for cluster_mapping in token_clusters:\n",
    "                if cluster_mapping is not None:\n",
    "                    # feature name should include which direction (left/right) and which clusters were found\n",
    "                    cluster_feature_name = '{0}_{1}_CL_{2}'.format(feature_prefix, map_prefix, str(cluster_mapping))\n",
    "                    features[cluster_feature_name] = True\n",
    "                    \n",
    "    def add_drug_window_features(self, feature_prefix, tokens, features):\n",
    "        found_in_window = False\n",
    "        for token in tokens:\n",
    "            # drug set is all lowercase\n",
    "            token_lower = token.lower()\n",
    "            if token_lower in self.drug_term_set:\n",
    "                found_in_window = True\n",
    "                self.found_drug_term_set.add(token_lower)\n",
    "                break\n",
    "                \n",
    "        if found_in_window:\n",
    "            drug_feature_name = 'D_{0}'.format(feature_prefix)\n",
    "            features[drug_feature_name] = True\n",
    "        \n",
    "    def add_local_window_cluster(self, prefix, token_idx, features, cluster_mapping_tokens_map):\n",
    "        if len(self.word_cluster_map_tuples) == 0:\n",
    "            return\n",
    "        \n",
    "        # iterate through the list of mappings already computed\n",
    "        for map_prefix, token_clusters in cluster_mapping_tokens_map.items():\n",
    "            cluster_mapping = token_clusters[token_idx]\n",
    "            if cluster_mapping is not None:\n",
    "                # feature name should include the position \n",
    "                cluster_feature_name = '{0}_{1}_CL'.format(prefix, map_prefix)\n",
    "                # and then the value will be the categorical cluster value\n",
    "                features[cluster_feature_name] = str(cluster_mapping)\n",
    "                \n",
    "    def add_cluster_bigrams(self, i, cluster_tokens_map, features):\n",
    "        # for each map, let's work over each window\n",
    "        for cluster_prefix, token_mappings in cluster_tokens_map.items():\n",
    "            # work across the window\n",
    "            for window_idx in range(-1 * self.window_size, self.window_size + 1):\n",
    "                # make sure that we do not go out of bounds\n",
    "                token_a_idx = i + window_idx\n",
    "                token_b_idx = i + window_idx + 1\n",
    "                if token_a_idx < 0 or token_b_idx >= len(token_mappings):\n",
    "                    continue\n",
    "                \n",
    "                #print('Working on token index {0}'.format(curr_token_idx))\n",
    "                token_a_prefix = str(window_idx)\n",
    "                token_b_prefix = str(window_idx + 1)\n",
    "                \n",
    "                feature_name = '{0}_G_{1}_{2}'.format(cluster_prefix, token_a_prefix, token_b_prefix)\n",
    "                feature_value = '{0}_{1}'.format(token_mappings[token_a_idx], token_mappings[token_b_idx])\n",
    "                features[feature_name] = feature_value\n",
    "                \n",
    "    def get_token_cluster(self, token, word_cluster_map):\n",
    "        # default to unknown\n",
    "        cluster_mapping = 'U'\n",
    "        if token in word_cluster_map:\n",
    "            cluster_mapping = word_cluster_map[token]\n",
    "        elif token.lower() in word_cluster_map:\n",
    "            cluster_mapping = word_cluster_map[token.lower()]\n",
    "            # if this was found as lower, let's save the original form with the same mapping to\n",
    "            # hopefully make the next time faster\n",
    "            word_cluster_map[token] = cluster_mapping\n",
    "            \n",
    "        return cluster_mapping\n",
    "    \n",
    "    def sent2features(self, sent):\n",
    "        sentence_features = []\n",
    "        \n",
    "        # TODO : This is more useful on documents which might be ALL UPPERCASE\n",
    "        use_lower_tokens = False\n",
    "        sentence_tokens_preferred = sent\n",
    "        if use_lower_tokens:\n",
    "            sentence_tokens_lower = [x.lower() for x in sent]\n",
    "            sentence_tokens_preferred = sentence_tokens_lower\n",
    "            \n",
    "        # POS tagging (if enabled)\n",
    "        sentence_pos_tag_pairs = None\n",
    "        if self.enable_pos_features:\n",
    "            sentence_pos_tag_pairs = self.pos_tagger.tag(sentence_tokens_preferred)\n",
    "            \n",
    "        # for each cluster map, let's pre-compute the mappings so we can do easily lookups later\n",
    "        cluster_mapping_tokens_map = {}\n",
    "        for word_cluster_map_tuple in self.word_cluster_map_tuples:\n",
    "            map_prefix = word_cluster_map_tuple[0]\n",
    "            word_cluster_map = word_cluster_map_tuple[1]\n",
    "            \n",
    "            token_clusters = []\n",
    "            \n",
    "            # now get the mapping for each token\n",
    "            for token in sentence_tokens_preferred:\n",
    "                cluster = self.get_token_cluster(token, word_cluster_map)\n",
    "                token_clusters.append(cluster)\n",
    "            \n",
    "            # save this\n",
    "            cluster_mapping_tokens_map[map_prefix] = token_clusters\n",
    "        \n",
    "        # now loop through each word in the sentence and create a feature vector (dictionary) for each\n",
    "        for i in range(len(sent)):\n",
    "            word = sent[i]\n",
    "        \n",
    "            # let's work across the window of tokens specified\n",
    "            features = {}\n",
    "            \n",
    "            # let's get tokens on either side\n",
    "            tokens_left = sentence_tokens_preferred[:i]\n",
    "            tokens_right = tokens_left = sentence_tokens_preferred[i + 1:]\n",
    "            \n",
    "            if self.enable_drug_term_features:\n",
    "                self.add_drug_window_features('L', tokens_left, features)\n",
    "                self.add_drug_window_features('R', tokens_right, features)\n",
    "                self.add_drug_window_features('ALL', sentence_tokens_preferred, features)\n",
    "\n",
    "            if self.enable_sentence_window_clusters:\n",
    "                self.add_sentence_window_clusters('L', tokens_left, features, cluster_mapping_tokens_map)\n",
    "                self.add_sentence_window_clusters('R', tokens_right, features, cluster_mapping_tokens_map)\n",
    "                \n",
    "            if self.enable_cluster_bigrams:\n",
    "                self.add_cluster_bigrams(i, cluster_mapping_tokens_map, features)\n",
    "\n",
    "            for window_idx in range(-1 * self.window_size, self.window_size + 1):\n",
    "                curr_token_idx = i + window_idx\n",
    "                #print('Working on token index {0}'.format(curr_token_idx))\n",
    "                prefix = str(window_idx)\n",
    "\n",
    "                if curr_token_idx < 0:\n",
    "                    if self.enable_lexical_features:\n",
    "                        features[prefix + 'wlow'] = 'bos'     \n",
    "                elif curr_token_idx >= len(sent):\n",
    "                    if self.enable_lexical_features:\n",
    "                        features[prefix + 'wlow'] = 'eos'\n",
    "                else:\n",
    "                    word = sentence_tokens_preferred[curr_token_idx]\n",
    "\n",
    "                    if self.enable_local_window_clusters:\n",
    "                        self.add_local_window_cluster(prefix, curr_token_idx, features, cluster_mapping_tokens_map)\n",
    "\n",
    "                    word_lower = word.lower()\n",
    "                    if self.enable_lexical_features:\n",
    "                        features[prefix + 'wlow'] = word_lower\n",
    "                        \n",
    "                        if self.enable_stem_features:\n",
    "                            features[prefix + 'wstem'] = self.stemmer.stem(word_lower)\n",
    "                        \n",
    "                        if 2 in self.char_ngrams_enabled:\n",
    "                            features[prefix + 'c[-2:]'] = word_lower[-2:]\n",
    "                            features[prefix + 'c[:2]'] = word_lower[:2]\n",
    "                        if 3 in self.char_ngrams_enabled:\n",
    "                            features[prefix + 'c[-3:]'] = word_lower[-3:]\n",
    "                            features[prefix + 'word[:3]'] = word_lower[:3]\n",
    "                        if 4 in self.char_ngrams_enabled:\n",
    "                            features[prefix + 'c[-4:]'] = word_lower[-4:]\n",
    "                            features[prefix + 'c[:4]'] = word_lower[:4]\n",
    "                            \n",
    "                    if self.enable_drug_term_features:\n",
    "                        # drug set is all lowercase\n",
    "                        if word_lower in drug_term_set:\n",
    "                            features[prefix + 'wdrug'] = True\n",
    "\n",
    "                    features[prefix + 'w.islower'] = word.islower()\n",
    "                    features[prefix + 'w.isupper'] = word.isupper()\n",
    "                    features[prefix + 'w.istitle'] = word.istitle()\n",
    "                    features[prefix + 'w.isdigit'] = word.isdigit()\n",
    "                    features[prefix + 'w.any_alpha'] = any(char.isalpha() for char in word_lower)\n",
    "                    features[prefix + 'w.all_punct'] = all(char in string.punctuation for char in word_lower)\n",
    "\n",
    "                    if self.enable_pos_features:\n",
    "                        postag_pair = sentence_pos_tag_pairs[curr_token_idx]\n",
    "                        # This pair will be in the form ('token', 'TAG')\n",
    "                        features[prefix + 'pos'] = postag_pair[1] \n",
    "\n",
    "                # update for any word encountered\n",
    "                #features.update(word_features)\n",
    "\n",
    "            sentence_features.append(features)\n",
    "        return sentence_features\n",
    "    \n",
    "print('CRFExtractor class ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes = [500, 5000, 10000]\n",
    "external_cluster_tuples = []\n",
    "pitt_cluster_tuples = []\n",
    "\n",
    "cluster_prefix_filepart_map = {'E' : 'wikipedia-pubmed-and-PMC-w2v', \n",
    "                               # NOTE : As noted in the README, these embeddings are not distributed so they\n",
    "                               # are commented out here\n",
    "                               #'P' : 'pubmed+wiki+pitts-nopunct-lower-cbow-n10'\n",
    "                              }\n",
    "\n",
    "for prefix, filepart in cluster_prefix_filepart_map.items():\n",
    "    for cluster_size in cluster_sizes:\n",
    "        full_prefix = '{0}_{1}'.format(prefix, cluster_size)\n",
    "        cluster_filename = 'WordClusters_K{0}_BatchKmeans_{1}.pickle'.format(cluster_size, filepart)\n",
    "        cluster_filepath = os.path.join(CLUSTERS_BASE_DIR, cluster_filename)\n",
    "        print('Loading clusters from : {}'.format(cluster_filepath))\n",
    "        with open(cluster_filepath, 'rb') as handle:\n",
    "            word_cluster_map = pickle.load(handle)\n",
    "            print('Word Cluster map size : {}'.format(len(word_cluster_map)))\n",
    "            \n",
    "            if prefix == 'E':\n",
    "                external_cluster_tuples.append(tuple((full_prefix, word_cluster_map)))\n",
    "            elif prefix == 'P':\n",
    "                pitt_cluster_tuples.append(tuple((full_prefix, word_cluster_map)))\n",
    "                \n",
    "print('DONE loading embeddings clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_term_set = set()\n",
    "with open(drug_term_path, 'r') as drug_file:\n",
    "    for line in drug_file:\n",
    "        line = line.strip()\n",
    "        drug_term_set.add(line)\n",
    "        \n",
    "print('Total Drug terms added : {}'.format(len(drug_term_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set each one of these to determine which final submission set we are writing out\n",
    "#FEATURE_SET = 'Fast'\n",
    "FEATURE_SET = 'Standard'\n",
    "#FEATURE_SET = 'Extended'\n",
    "\n",
    "RUN_NAME = 'Run1'\n",
    "#RUN_NAME = 'Run2'\n",
    "#RUN_NAME = 'SGD'\n",
    "#RUN_NAME = 'AP'\n",
    "#RUN_NAME = 'PA'\n",
    "#RUN_NAME = 'AROW'\n",
    "\n",
    "if FEATURE_SET == 'Fast':\n",
    "    print('Prepping fast extractor for rapid development...')\n",
    "    extractor = CRFExtractor(\n",
    "                            char_ngrams_enabled = set(),\n",
    "                            word_cluster_map_tuples = [],\n",
    "                            #word_cluster_map_tuples = external_cluster_tuples + pitt_cluster_tuples,\n",
    "                            enable_pos_features = False,\n",
    "                            enable_lexical_features = False,\n",
    "                            pos_tagger = None,\n",
    "                            enable_local_window_clusters = False,\n",
    "                            enable_sentence_window_clusters = False,\n",
    "                            enable_cluster_bigrams = False,\n",
    "                            drug_term_set = set(),\n",
    "                            enable_drug_term_features = False,\n",
    "                            window_size = 0)\n",
    "elif FEATURE_SET == 'Standard':\n",
    "    print('Prepping better Standard extractor')\n",
    "    extractor = CRFExtractor(\n",
    "                            #char_ngrams_enabled = set([2, 3, 4]), \n",
    "                            char_ngrams_enabled = set([2, 3]), \n",
    "                            #word_cluster_map_tuples = external_cluster_tuples,\n",
    "                            word_cluster_map_tuples = pitt_cluster_tuples,\n",
    "                            #word_cluster_map_tuples = external_cluster_tuples + pitt_cluster_tuples,\n",
    "                            enable_pos_features = True,\n",
    "                            enable_lexical_features = True,\n",
    "                            pos_tagger = brill_tagger,\n",
    "                            enable_local_window_clusters = True,\n",
    "                            enable_sentence_window_clusters = False,\n",
    "                            enable_cluster_bigrams = True,\n",
    "                            drug_term_set = set(),\n",
    "                            enable_drug_term_features = False,\n",
    "                            window_size = 2\n",
    "                            )\n",
    "    \n",
    "elif FEATURE_SET == 'Extended':\n",
    "    print('Prepping better Extended extractor')\n",
    "    extractor = CRFExtractor(\n",
    "                            #char_ngrams_enabled = set([2, 3, 4]), \n",
    "                            char_ngrams_enabled = set([2, 3]), \n",
    "                            #word_cluster_map_tuples = external_cluster_tuples,\n",
    "                            #word_cluster_map_tuples = pitt_cluster_tuples,\n",
    "                            word_cluster_map_tuples = external_cluster_tuples + pitt_cluster_tuples,\n",
    "                            enable_pos_features = True,\n",
    "                            enable_lexical_features = True,\n",
    "                            pos_tagger = brill_tagger,\n",
    "                            enable_local_window_clusters = True,\n",
    "                            enable_sentence_window_clusters = False,\n",
    "                            enable_cluster_bigrams = True,\n",
    "                            drug_term_set = drug_term_set,\n",
    "                            enable_drug_term_features = True,\n",
    "                            window_size = 2\n",
    "                            )\n",
    "\n",
    "if False:\n",
    "    print('Example features : ')\n",
    "    example_features = [extractor.sent2features(s) for s in X_train[0]]\n",
    "    print(example_features)\n",
    "\n",
    "print('Extracting features...')\n",
    "X_train_feat = [extractor.sent2features(s) for s in X_train]\n",
    "X_test_feat = [extractor.sent2features(s) for s in X_test]\n",
    "print('DONE extracting features...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for drug_term in sorted(list(extractor.found_drug_term_set)):\n",
    "    print(drug_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_feat[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SINGLE_FOLD_FIT = True\n",
    "WRITE_TEST_PREDICTIONS = True\n",
    "TRAIN_ON_ALL_DATA = True\n",
    "\n",
    "c1 = 0.0\n",
    "c2 = 0.0\n",
    "crf_algorithm = 'lbfgs'\n",
    "\n",
    "if RUN_NAME == 'Run1':\n",
    "    # this set allowed F1 of 86.8 on the TEST set (3/4/18 at 9pm)\n",
    "    c1 = 0.1\n",
    "    c2 = 0.5\n",
    "elif RUN_NAME == 'Run2':\n",
    "    # this set allowed a different F1 of 86.5 on the TEST set (3/5/18 at 7am):\n",
    "    c1 = 0.2\n",
    "    c2 = 0.75\n",
    "elif RUN_NAME == 'SGD':\n",
    "    # this set allowed a different F1 of 86.5 on the TEST set (3/5/18 at 7am):\n",
    "    # this set allowed F1 of 86.8 on the TEST set (3/4/18 at 9pm)\n",
    "    c1 = None\n",
    "    c2 = 0.5\n",
    "    crf_algorithm = 'l2sgd'\n",
    "elif RUN_NAME == 'AP':\n",
    "    c1 = None\n",
    "    c2 = None\n",
    "    crf_algorithm = 'ap'\n",
    "elif RUN_NAME == 'PA':\n",
    "    c1 = None\n",
    "    c2 = None\n",
    "    crf_algorithm = 'pa'\n",
    "elif RUN_NAME == 'AROW':\n",
    "    c1 = None\n",
    "    c2 = None\n",
    "    crf_algorithm = 'arow'\n",
    "    \n",
    "max_iterations = 100\n",
    "\n",
    "if FEATURE_SET == 'Fast':\n",
    "    max_iterations = 5\n",
    "    \n",
    "print('Set values for {}'.format(RUN_NAME))\n",
    "print('L1/L2 Values : c1 = {0}, c2 = {1}'.format(c1, c2))\n",
    "print('Algorithm being used for training : {}'.format(crf_algorithm))\n",
    "print('max_iterations = {0}'.format(max_iterations))\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm=crf_algorithm,\n",
    "    # these get set above based on the run\n",
    "    c1 = c1,\n",
    "    c2 = c2,\n",
    "    max_iterations = max_iterations,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "if SINGLE_FOLD_FIT:\n",
    "    print('Training CRF...')\n",
    "\n",
    "    if WRITE_TEST_PREDICTIONS and TRAIN_ON_ALL_DATA:\n",
    "        print('Training using all available data...')\n",
    "        crf.fit(X_train_feat + X_test_feat, y_train + y_test)\n",
    "    else:\n",
    "        print('Training using training data only...')\n",
    "        crf.fit(X_train_feat, y_train)\n",
    "\n",
    "    print('DONE Training CRF...')\n",
    "else:\n",
    "    print('Skipping single fold training')\n",
    "    \n",
    "print(crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "\n",
    "\n",
    "model_dir = 'models/CRF_{0}_{1}_{2}'.format(FEATURE_SET, RUN_NAME, today.strftime('%m_%d_%Y'))\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "model_path = '{0}/crf.pkl'.format(model_dir)\n",
    "\n",
    "print(model_path)\n",
    "\n",
    "joblib.dump(crf, model_path) \n",
    "\n",
    "print('Wrote model to : [{0}]'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "label_set = set()\n",
    "\n",
    "for instance_labels in (y_train + y_test):\n",
    "    label_set |= set(instance_labels)\n",
    "    \n",
    "labels = list(label_set)\n",
    "\n",
    "labels.remove('O')\n",
    "print(label_set)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "\n",
    "# let's get a coarse report as well\n",
    "sorted_coarse_labels = sorted(\n",
    "    set(get_coarse_labels(labels)),\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "\n",
    "print(sorted_labels)\n",
    "print(sorted_coarse_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SINGLE_FOLD_FIT:\n",
    "    y_pred = crf.predict(X_test_feat)\n",
    "    sklearn_crfsuite.metrics.flat_f1_score(y_test, y_pred,\n",
    "                          average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SINGLE_FOLD_FIT:\n",
    "    y_train_pred = crf.predict(X_train_feat)\n",
    "    sklearn_crfsuite.metrics.flat_f1_score(y_train, y_train_pred,\n",
    "                          average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SINGLE_FOLD_FIT:\n",
    "    print(sklearn_crfsuite.metrics.flat_classification_report(\n",
    "        y_test, y_pred, labels=sorted_labels, digits=3\n",
    "    ))\n",
    "    \n",
    "    #print(len(y_test))\n",
    "    #print(len(y_pred))\n",
    "    \n",
    "    print(sklearn_crfsuite.metrics.flat_classification_report(\n",
    "        [get_coarse_labels(x) for x in y_test], [get_coarse_labels(x) for x in y_pred], labels=sorted_coarse_labels, digits=3\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SINGLE_FOLD_FIT:\n",
    "    sorted_coarse_no_filter = list(sorted_coarse_labels)\n",
    "    sorted_coarse_no_filter.append('O')\n",
    "    sorted_coarse_no_filter = sorted(sorted_coarse_no_filter)\n",
    "    print(sorted_coarse_no_filter)\n",
    "\n",
    "    confusion_coarse = confusion_matrix(flatten([get_coarse_labels(x) for x in y_test]), \n",
    "                                        flatten([get_coarse_labels(x) for x in y_pred])\n",
    "                                        , labels = sorted_coarse_no_filter)\n",
    "    plt.figure()\n",
    "\n",
    "    plot_confusion_matrix(confusion_coarse, classes=sorted_coarse_no_filter,\n",
    "                          title='Coarse Label Confusion for : [{0}]'.format('TEST'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's try to evaluate spans just as the final BIOC evaluation will happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SINGLE_FOLD_FIT:\n",
    "    evaluate_via_bioc(test_docs, crf, extractor, PREDICTION_DIR, MADE_BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "Let's take a look at some cases of opportunities for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_string(tokens, tags):\n",
    "    tagged_string = ''\n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        tag = tags[i]\n",
    "        tagged_string += token\n",
    "        if tag != 'O':\n",
    "            tagged_string += '/{0}'.format(tag)\n",
    "            \n",
    "        tagged_string += ' '\n",
    "        \n",
    "    return tagged_string.strip()\n",
    "\n",
    "if SINGLE_FOLD_FIT:\n",
    "\n",
    "    # set up a map to collect errors\n",
    "    test_fp_map = {}\n",
    "    test_fn_map = {}\n",
    "    test_tp_map = {}\n",
    "    for label in labels:\n",
    "        label = label.replace('B-', '').replace('I-', '')\n",
    "        test_fp_map[label] = set()\n",
    "        test_fn_map[label] = set()\n",
    "        test_tp_map[label] = set()\n",
    "\n",
    "    # then gather the errors by label\n",
    "    for i in range(len(y_test)):\n",
    "        sentence_tokens = X_test[i]\n",
    "        sentence_gold = y_test[i]\n",
    "        sentence_pred = y_pred[i]\n",
    "        sentence_gold = [x.replace('B-', '').replace('I-', '') for x in sentence_gold]\n",
    "        sentence_pred = [x.replace('B-', '').replace('I-', '') for x in sentence_pred]\n",
    "        sentence_gold_str = 'GOLD : ' + create_sentence_string(sentence_tokens, sentence_gold)\n",
    "        sentence_pred_str = 'PRED : ' + create_sentence_string(sentence_tokens, sentence_pred)\n",
    "        #print(sentence_gold_str)\n",
    "        #print(sentence_pred_str)\n",
    "\n",
    "        pair = tuple((sentence_gold_str, sentence_pred_str))\n",
    "\n",
    "        for tag_idx in range(len(sentence_gold)):\n",
    "            pred_tag = sentence_pred[tag_idx]\n",
    "            gold_tag = sentence_gold[tag_idx]\n",
    "\n",
    "            if pred_tag == gold_tag:\n",
    "                if pred_tag != 'O':\n",
    "                    test_tp_map[pred_tag].add(pair)\n",
    "            else:\n",
    "                if pred_tag != 'O':\n",
    "                    test_fp_map[pred_tag].add(pair)\n",
    "                if gold_tag != 'O':\n",
    "                    test_fn_map[gold_tag].add(pair)\n",
    "\n",
    "        #break\n",
    "\n",
    "    print('Total TP examples : {}'.format(np.sum([len(test_tp_map[tag]) for tag in test_tp_map.keys()])))\n",
    "    print('Total FP examples : {}'.format(np.sum([len(test_fp_map[tag]) for tag in test_fp_map.keys()])))\n",
    "    print('Total FN examples : {}'.format(np.sum([len(test_fn_map[tag]) for tag in test_fn_map.keys()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error_comparisons(pairs_set, error_type):\n",
    "    print('**************')\n",
    "    print(error_type)\n",
    "    for pair in pairs_set:\n",
    "        print(pair[0])\n",
    "        print(pair[1])\n",
    "        print('')\n",
    "    print('**************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_ERROR_COMPARISONS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's enumerate opportunities\n",
    "if PRINT_ERROR_COMPARISONS:\n",
    "    print_error_comparisons(test_fn_map['ADE'], 'False Negatives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's enumerate opportunities\n",
    "if PRINT_ERROR_COMPARISONS:\n",
    "    print_error_comparisons(test_fp_map['ADE'], 'False Positives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's enumerate opportunities\n",
    "if PRINT_ERROR_COMPARISONS:\n",
    "    print_error_comparisons(test_fn_map['Indication'], 'False Negatives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's enumerate opportunities\n",
    "if PRINT_ERROR_COMPARISONS:\n",
    "    print_error_comparisons(test_fp_map['Indication'], 'False Positives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's enumerate opportunities\n",
    "if PRINT_ERROR_COMPARISONS:\n",
    "    print_error_comparisons(test_fn_map['SSLIF'], 'False Negatives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's enumerate opportunities\n",
    "if PRINT_ERROR_COMPARISONS:\n",
    "    print_error_comparisons(test_fp_map['SSLIF'], 'False Positives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "TOP_N_FEATURES = 50 \n",
    "\n",
    "if SINGLE_FOLD_FIT:\n",
    "        \n",
    "    print(\"Top positive:\")\n",
    "    print_state_features(Counter(crf.state_features_).most_common(TOP_N_FEATURES))\n",
    "\n",
    "    print(\"\\nTop negative:\")\n",
    "    print_state_features(Counter(crf.state_features_).most_common()[-1 * TOP_N_FEATURES:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionally run a full parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "PERFORM_PARAM_SEARCH = False\n",
    "\n",
    "MULTICORE_PARAM_FOLDS = 5\n",
    "# each fold can take 2 to 2.5 GB, so with the parent process we can only fit 4 folds into 16 GB\n",
    "MULTICORE_PARAM_JOBS = 1\n",
    "MULTICORE_PARAM_ITERATIONS = 3\n",
    "\n",
    "rs = None\n",
    "\n",
    "if PERFORM_PARAM_SEARCH:\n",
    "    print('Starting Param Search...')\n",
    "    \n",
    "    params_space = {\n",
    "        #'c1' : expon(scale = 0.5), # search range in sklearn_crfsuite example\n",
    "        #'c2' : expon(scale = 0.05) \n",
    "        'c1' : [0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1.0, 5.0, 10.0],\n",
    "        'c2' : [0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1.0, 5.0, 10.0]\n",
    "    }\n",
    "    \n",
    "    f1_scorer = make_scorer(metrics.flat_f1_score, average = 'weighted', labels = labels)\n",
    "    \n",
    "    rs = RandomizedSearchCV(crf, params_space, \n",
    "                                   cv = MULTICORE_PARAM_FOLDS,\n",
    "                                   verbose = 1,\n",
    "                                   n_jobs = MULTICORE_PARAM_JOBS,\n",
    "                                   n_iter = MULTICORE_PARAM_ITERATIONS,\n",
    "                                   random_state = int(time.time() % 100),\n",
    "                                   scoring = f1_scorer)\n",
    "\n",
    "    rs.fit(X_train_feat, y_train)\n",
    "\n",
    "    print('Best params : {}'.format(rs.best_params_))\n",
    "    print('Best CV score : {}'.format(rs.best_score_))\n",
    "    print('model size : {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "\n",
    "    best_model = rs.best_estimator_\n",
    "\n",
    "    validation_start_time = time.time()\n",
    "\n",
    "    print('Now performing additional validation')\n",
    "\n",
    "    #f1 = _score(estimator, X_test, y_test, scorer, is_multimetric=False)\n",
    "    test_pred_unflattened = best_model.predict(X_test_feat)\n",
    "\n",
    "    print('**********')\n",
    "    #print('AUC : {0}'.format(auc))\n",
    "    #print('TEST F1 : {0}'.format(f1))\n",
    "    #print('TEST Precision : {0}'.format(precision))\n",
    "    #print('TEST Recall : {0}'.format(recall))\n",
    "\n",
    "    print('Report for CV (TRAIN):')\n",
    "    # use these flattened versions\n",
    "    print(sklearn_crfsuite.metrics.flat_classification_report(\n",
    "                    [get_coarse_labels(x) for x in y_train], \n",
    "                     [get_coarse_labels(x) for x in best_model.predict(X_train_feat)],\n",
    "                    labels = sorted_coarse_labels, digits = 3))\n",
    "\n",
    "    print('Report for CV (TEST):')\n",
    "    # use these flattened versions\n",
    "    print(sklearn_crfsuite.metrics.flat_classification_report(\n",
    "                    [get_coarse_labels(x) for x in y_test], \n",
    "                     [get_coarse_labels(x) for x in test_pred_unflattened],\n",
    "                    labels = sorted_coarse_labels, digits = 3))\n",
    "\n",
    "    print('Total time for validation : {} seconds'.format(time.time() - validation_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of params from RandomSearch...\n",
    "if rs is not None:\n",
    "    print('Preparing plot of RandomSearch results...')\n",
    "    _x = [s.parameters['c1'] for s in rs.grid_scores_]\n",
    "    _y = [s.parameters['c2'] for s in rs.grid_scores_]\n",
    "    _c = [s.mean_validation_score for s in rs.grid_scores_]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(12, 12)\n",
    "    ax = plt.gca()\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('C1')\n",
    "    ax.set_ylabel('C2')\n",
    "    ax.set_title(\"Randomized Hyperparameter Search CV Results (F1) (min={:0.3}, max={:0.3})\".format(\n",
    "        min(_c), max(_c)\n",
    "    ))\n",
    "\n",
    "    ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
    "\n",
    "    print(\"Dark blue => {:0.4} F1, dark red => {:0.4} F1\".format(min(_c), max(_c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, if enabled, we will write out the appropriate predictions based on the features and the run for final submission for the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WRITE_TEST_PREDICTIONS:\n",
    "    print('Preparing to write HOLD OUIT predictions...')\n",
    "    TEST_PRED_BASE_DIR = 'final_predictions'\n",
    "    TEST_SOURCE_SUBDIR = 'made1.0_task1_3_test'\n",
    "    TEST_SOURCE_DIR = r'{0}/{1}'.format(MADE_ALL_SETS_BASE_DIR, TEST_SOURCE_SUBDIR)\n",
    "    test_pred_run_subdir = r'{0}/Task1-{1}-{2}'.format(TEST_PRED_BASE_DIR, FEATURE_SET, RUN_NAME)\n",
    "    print('Prepping to write predictions to : {}'.format(test_pred_run_subdir))\n",
    "    \n",
    "    print('Reading unlabeled source files from : {}'.format(TEST_SOURCE_DIR))\n",
    "    holdout_test_docs = read_made_data(TEST_SOURCE_DIR, made_tokenizer)\n",
    "\n",
    "    print('Total Final Holdout Docs : {}'.format(len(holdout_test_docs)))\n",
    "    \n",
    "    # pass None as the MADE base dir since there are no gold labels for comparison\n",
    "    evaluate_via_bioc(holdout_test_docs, crf, extractor, test_pred_run_subdir, None)\n",
    "    \n",
    "    print('DONE writing out HOLDOUT predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE : The cell before is for the ACTUAL finally released test set (released after the challenge and workshop were both completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WRITE_TEST_PREDICTIONS:\n",
    "    print('Preparing to write RELEASED TEST SET predictions...')\n",
    "    TEST_SET_PRED_BASE_DIR = 'test_set_predictions'\n",
    "    TEST_SET_SOURCE_SUBDIR = 'made_test_data'\n",
    "    TEST_SET_SOURCE_DIR = r'{0}/{1}'.format(MADE_ALL_SETS_BASE_DIR, TEST_SET_SOURCE_SUBDIR)\n",
    "    test_set_pred_run_subdir = r'{0}/Task1-{1}-{2}'.format(TEST_SET_PRED_BASE_DIR, FEATURE_SET, RUN_NAME)\n",
    "    print('Prepping to write predictions to : {}'.format(test_set_pred_run_subdir))\n",
    "    \n",
    "    print('Reading TEST SET source files from : {}'.format(TEST_SET_SOURCE_DIR))\n",
    "    test_set_docs = read_made_data(TEST_SET_SOURCE_DIR, made_tokenizer)\n",
    "\n",
    "    print('Total Final TEST SET Docs : {}'.format(len(test_set_docs)))\n",
    "    \n",
    "    # pass None as the MADE base dir TO ASSUME that there are no gold labels for comparison\n",
    "    evaluate_via_bioc(test_set_docs, crf, extractor, test_set_pred_run_subdir, None)\n",
    "    \n",
    "    print('DONE writing out TEST SET predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_ON_ALL_GOLD_DATA = True\n",
    "if PREDICT_ON_ALL_GOLD_DATA:\n",
    "    print('Preparing to write ALL GOLD predictions...')\n",
    "    GOLD_PRED_BASE_DIR = 'all_gold_predictions'\n",
    "\n",
    "    print('Total Gold Prediction Docs : {}'.format(len(train_docs + test_docs)))\n",
    "    \n",
    "    # pass None as the MADE base dir since there are no gold labels for comparison\n",
    "    evaluate_via_bioc(train_docs + test_docs, crf, extractor, GOLD_PRED_BASE_DIR, None)\n",
    "    \n",
    "    print('DONE writing out ALL GOLD predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
